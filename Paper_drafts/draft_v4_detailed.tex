\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage[numbers]{natbib}
\usepackage{url}

\title{Enhancing Vision-Language Models for Chess Position Understanding via Symbolic Grounding}
\author{Vivek Padman, Prajeet Khante, Hirak Basumatory\\
\small Paper under double-blind review}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Vision-Language Models (VLMs) have shown remarkable capabilities in visual reasoning tasks, yet they struggle with precise spatial understanding required for structured domains like chess. We propose a two-phase approach to bridge this vision--text modality gap by grounding visual inputs in symbolic representations (FEN notation). First, we demonstrate via a **retrieval-based** method that providing ground-truth or retrieved FEN strings significantly enhances VLM performance, achieving 99.98\% top-1 accuracy and a 75\% relative improvement in chess understanding tasks. Second, to address the limitations of retrieval in open-world settings, we explore a **Generative FEN Decoder**—a fine-tuned CLIP-based encoder-decoder model that synthesizes FEN strings for unseen board configurations. While the generative approach faces significant challenges (0\% exact match accuracy due to exposure bias and premature stopping), our analysis reveals important lessons for vision-to-symbol generation. We demonstrate that providing VLMs with symbolic context significantly improves performance on tasks requiring precise piece localization and board state reasoning. Our findings suggest that hybrid vision--symbolic approaches can enhance VLM performance in structured visual domains, though generative methods require further research to overcome current limitations.
\end{abstract}

\noindent\textbf{Keywords:} Vision-Language Models, Chess AI, Symbolic Reasoning, CLIP, Multimodal Learning, Generative Decoding

\section{Introduction}

Vision-Language Models (VLMs) such as LLaVA and GPT-4V have demonstrated impressive capabilities across diverse visual reasoning tasks. However, they face challenges in domains requiring precise spatial understanding and symbolic reasoning, such as chess position analysis. While humans can easily translate a chess board image into Forsyth-Edwards Notation (FEN) --- a standard textual representation --- VLMs often struggle with this task due to the fine-grained spatial reasoning required.

Chess has rapidly become a standard testbed for probing the reasoning and planning capabilities of large language models (LLMs) and multimodal systems. Recent work shows that purely text-based LLMs trained on complete game trajectories can reach strong-amateur or even master-level strength when games are represented in FEN or UCI text formats.\footnote{E.g., ChessLLM reaches $\sim$1788 Elo against Stockfish when trained on over 20B chess tokens~\citep{zhang2025chessllm}.} Similarly, explanation-enriched datasets such as MATE demonstrate that adding strategic and tactical natural-language rationales substantially improves move-selection accuracy, surpassing strong commercial LLMs~\citep{wang2024mate}. At the same time, other work targets human move prediction~\citep{kreiger2024predict}, human-aligned play and skill calibration~\citep{zhang2024allie}, and purely searchless transformer policies trained on massive supervised datasets~\citep{deepmind2024searchless}.

Yet, despite this progress, most chess-LLM work assumes a \emph{textual} representation of the position is already available (PGN, FEN, UCI). In many realistic settings, the input is visual: a rendered board from a GUI, a video frame, or a real-world camera image. In these cases, the bottleneck is not the downstream reasoning but the \emph{vision-to-symbol} mapping that provides a clean, unambiguous board state.

Chess therefore serves as an ideal testbed for studying VLM limitations because:
\begin{itemize}[leftmargin=*]
    \item \textbf{Precise spatial reasoning:} Distinguishing between similar pieces (e.g., white pawn vs.\ white bishop) and subtle configuration differences requires fine-grained visual understanding.
    \item \textbf{Symbolic grounding:} Chess has a well-defined symbolic language (FEN, algebraic notation) that supports discrete decision-making and formal reasoning.
    \item \textbf{Objective evaluation:} Ground truth board states, legal moves, and engine evaluations can be computed programmatically.
\end{itemize}

In this work, we investigate the hypothesis that **symbolic grounding**—explicitly converting the visual input into a structured text representation—can unlock the reasoning capabilities of strong LLMs. We present our contributions in two phases:

1.  **Phase 1: Retrieval-Based Grounding.** We first validate the utility of FEN context using a fine-tuned CLIP model to retrieve FEN strings from a database. This serves as a proof-of-concept that *if* a VLM has access to accurate symbolic state, its reasoning performance improves dramatically (75\% relative improvement).
2.  **Phase 2: Generative FEN Prediction.** To overcome the "closed-world" limitation of retrieval (which fails on unseen positions), we explore a **Generative FEN Decoder**. This model combines our fine-tuned CLIP encoder with a Transformer decoder to synthesize FEN strings for unseen board configurations. While this approach faces significant challenges (exposure bias, premature stopping), our systematic evaluation provides important insights into the difficulties of vision-to-symbol generation and suggests directions for future research.

Our approach is instantiated in the open-source \emph{Vichar-CLIP} codebase,\footnote{\url{https://github.com/vieveks/vichar_clip_paper_cursor}.} which trains a ViT-B/32 CLIP model on chess boards to predict FEN and achieves $>96\%$ top-1 accuracy on held-out positions.

\paragraph{Contributions.} Our contributions are:
\begin{itemize}[leftmargin=*]
    \item \textbf{Novel pipeline:} We introduce, to our knowledge, the first VLM pipeline that explicitly augments vision with symbolic representations (FEN) for structured visual reasoning in chess, demonstrating significant improvements when accurate FEN is available.
    \item \textbf{Retrieval-based validation:} We achieve 99.98\% top-1 accuracy on FEN retrieval, demonstrating that fine-tuned CLIP models are highly effective for closed-world vision-to-symbol mapping.
    \item \textbf{Generative architecture exploration:} We propose and systematically evaluate an Encoder-Decoder architecture for generating FEN strings for unseen positions, revealing fundamental challenges (exposure bias, training-inference mismatch) that limit current generative approaches.
    \item \textbf{Empirical validation:} We demonstrate a 75\% relative improvement on chess understanding benchmarks when accurate FEN context is provided, compared to a vision-only baseline.
    \item \textbf{Lessons for vision-to-symbol generation:} Our analysis of generative model failures provides important insights into the challenges of structured output generation from visual inputs, including the disconnect between training metrics and generation quality, and the critical role of exposure bias.
    \item \textbf{Link to chess-LLM literature:} We situate our method within recent work on chess-specific LLMs~\citep{zhang2025chessllm,feng2023chessgpt,jiang2023nlchess}, reasoning with explanations~\citep{wang2024mate}, searchless transformers~\citep{deepmind2024searchless}, world models~\citep{xie2025worldmodels}, and human-aligned engines~\citep{zhang2024allie}.
    \item \textbf{Open-source:} We release our training and evaluation pipeline (Vichar-CLIP) and the benchmark scripts necessary to reproduce our results.
\end{itemize}

\section{Related Work}

\subsection{Vision-Language Models and CLIP}

Contrastive vision-language pretraining, as popularized by CLIP~\citep{radford2021clip}, learns a joint embedding space for images and text by predicting matching pairs from large-scale web data. This approach has proven remarkably effective for zero-shot classification, retrieval, and transfer across a wide range of visual domains. Subsequent work has focused on making CLIP more efficient (e.g., patch masking and patch selection strategies~\citep{pei2025clippgs}) and domain-adapting CLIP-like models to specialized settings.

Our method builds on the CLIP paradigm but extends it to **generative tasks**. While standard CLIP is excellent for retrieval, it cannot produce novel text strings for unseen visual configurations. We address this by attaching a Transformer decoder to the CLIP image encoder, allowing us to generate precise FEN strings token-by-token. This mirrors approaches in image captioning (e.g., CoCa, GIT) but specialized for the rigid syntax of chess notation.

\subsection{Chess and LLMs}

A growing body of work treats chess as a structured testbed for LLM reasoning:

\paragraph{Text-only chess LLMs.} ChessLLM~\citep{zhang2025chessllm} fine-tunes an LLM on over 20B tokens of chess games represented in FEN and move text, achieving an Elo of $\sim$1788 against Stockfish and demonstrating that high-quality, full-game supervision significantly improves strength. ChessBench and related work on amortized planning with transformers~\citep{deepmind2024searchless} show that supervised transformers trained on engine-annotated data can reach grandmaster-level performance without explicit search by predicting state- and action-values.

ChessGPT~\citep{feng2023chessgpt} bridges policy learning and language modeling through a large-scale multimodal chess dataset, introducing ChessGPT and ChessCLIP as models that jointly handle game state, policy, and language commentary. The MATE dataset~\citep{wang2024mate} further shows that enriching positions with strategy and tactic explanations yields up to $\sim$95\% move-selection accuracy and even boosts the performance of powerful black-box LLMs when explanations are provided as context.

\paragraph{Human-like and human-aligned play.} Parallel work focuses on modeling human behavior rather than optimal engine play. ALLIE~\citep{zhang2024allie} trains a multi-headed transformer to predict moves, pondering times, and resignation decisions from human game logs, achieving state-of-the-art human move prediction and near-perfect skill calibration (average gap $\sim$49 Elo). Other work explicitly targets move prediction for a single player~\citep{kreiger2024predict} or subsets of the rating spectrum~\citep{maia2020,skidanov2025behaviour}. These efforts show that LLM-like architectures can capture rich human behavior when provided with accurate textual state representations.

\paragraph{Natural-language chess engines.} Smaller models, such as the GPT-2-XL system of Jiang~\citep{jiang2023nlchess}, explore training language models to answer natural-language chess queries (e.g., legality, piece identity, evaluation) and to generate moves using instruction fine-tuning and chain-of-thought prompting. While these models exhibit promising qualitative reasoning, their move quality often lags behind specialized engines.

Our work is complementary: we assume a strong, general-purpose VLM or LLM for downstream reasoning but focus on the \emph{visual-to-symbolic} step that almost all prior work sidesteps by assuming FEN or PGN is already given.

\subsection{Chess, NLP, and Explanations}

Beyond move prediction and engine strength, several works study chess through the lens of NLP:

\begin{itemize}[leftmargin=*]
    \item \textbf{Policy + language integration:} ChessGPT~\citep{feng2023chessgpt} unifies policy learning with language modeling across game records, commentary, and instructions.
    \item \textbf{Sentiment and commentary:} SentiMATE~\citep{kamlish2019sentimate} and later ABSA-style models analyze sentiment in chess commentary to evaluate moves. Recent work extends aspect-based sentiment analysis (ABSA) to chess textbooks using player-predicate-move triples and RoBERTa-based classifiers, showing that sentiment over textual descriptions correlates meaningfully with engine evaluations.
    \item \textbf{Human move prediction with LLMs:} Kreiger~\citep{kreiger2024predict} evaluates GPT-family models and chess-specific engines on predicting human moves, finding that prompt engineering improves LLM performance but chess-specific models such as Maia still dominate.
\end{itemize}

These works support our claim that \emph{textual} representations of chess positions are powerful interfaces for LLMs. Our goal is to reliably produce such representations from images, so that text-based techniques (sentiment, explanations, policy learning) become available in purely visual settings.

\subsection{World Models, Games, and Multi-Agent Reasoning}

There is a broader trend of framing interactive environments as sequence modeling problems over text. World-model work in text-based games and synthetic environments demonstrates that transformers can approximate environment dynamics and rewards purely from logged trajectories, then serve as simulators, policies, or planners~\citep{wang2024worldsim,xie2025worldmodels}. WordPlay-style benchmarks and ByteSized corpora treat game state and actions as text, showing strong generalization and planning capabilities in interactive narratives.

In multi-agent settings, Explicit Models of Opponents (EMO)~\citep{yu2025emo} build individual LLM-based opponent models with bi-level feedback, significantly improving win rates and role inference in social deduction games. Meanwhile, narrative-focused frameworks such as PANGeA~\citep{buongiorno2024pangea} show how LLMs with memory and validation modules can maintain coherent, rule-abiding narratives in RPGs.

Chess sits at the intersection of these lines of work: it is both a complex planning problem and a social setting where human-aligned reasoning and opponent modeling matter. Our method can be viewed as providing a high-fidelity \emph{symbolic world state} (FEN) from pixels, enabling downstream world-model or multi-agent techniques to operate on structured input.

\subsection{Symbolic-Neural Hybrid Systems}

Neuro-symbolic AI aims to combine differentiable perception with discrete symbolic reasoning. Prior work in chess includes engines that incorporate sentiment or textual knowledge into evaluation functions~\citep{kamlish2019sentimate} and systems that use logic or knowledge graphs to augment neural policies.

Our approach is deliberately simple: we do not perform explicit symbolic search or theorem proving. Instead, we use symbolic representations as an \emph{input modality} to a large VLM. This follows the spirit of recent work that shows explanations and textual structure can scaffold better reasoning in LLMs~\citep{wang2024mate}, but applied to a vision-to-symbol pipeline.

\section{Method}

We propose a pipeline that augments a general-purpose VLM with a specialized "Vision-to-FEN" module. We explore two architectures for this module.

\subsection{Approach 1: Retrieval-Based FEN Matching}
\label{subsec:retrieval}

In our initial investigation, we treat FEN identification as an image--text retrieval problem.

\paragraph{Architecture.} We fine-tune a **CLIP ViT-B/32** model on pairs of (Board Image, FEN String).
\paragraph{Training.} We use the standard contrastive loss:
\begin{equation}
    \mathcal{L} = - \log \frac{\exp\left(\mathrm{sim}(I_i, T_i) / \tau\right)}{\sum_{j} \exp\left(\mathrm{sim}(I_i, T_j) / \tau\right)},
\end{equation}
where $I_i$ is the image embedding, $T_i$ is the FEN text embedding, $\mathrm{sim}(\cdot,\cdot)$ denotes cosine similarity, and $\tau$ is a temperature parameter, following CLIP~\citep{radford2021clip}.

\paragraph{Inference.} Given a query image, we compute its embedding and retrieve the nearest neighbor from a pre-computed index of FEN strings.
\paragraph{Limitation.} This method is highly accurate for positions that exist in the database (e.g., common opening lines or specific puzzles) but fails for novel positions not in the index. It serves, however, to empirically prove that *retrieved* symbolic context helps the VLM.

\subsection{Approach 2: Generative FEN Prediction}
\label{subsec:generative}

To address the closed-world limitation, we develop a generative model capable of synthesizing FEN strings for unseen positions.

\paragraph{Architecture.}
\begin{enumerate}
    \item **Encoder:** The **CLIP ViT-B/32** vision model from Approach 1, initialized with weights fine-tuned in the retrieval phase. We extract spatial patch embeddings ($7 \times 7 = 49$ tokens) rather than pooled features to preserve spatial information needed for piece localization.
    \item **Decoder:** A **Transformer Decoder** (6 layers, 8 heads, 512 dimensions) that attends to the encoder's spatial tokens and autoregressively generates the FEN string token-by-token using a character-level tokenizer.
\end{enumerate}

\paragraph{Two-Stage Training Strategy.}
\begin{itemize}
    \item **Stage 1 (Syntax Learning):** We **freeze** the CLIP encoder and train only the Decoder. This teaches the model the grammar of FEN notation (e.g., rank separators `/`, empty square numbers) while leveraging fixed visual features.
    \item **Stage 2 (Fine-Tuning):** We **unfreeze** the encoder and fine-tune the full model end-to-end. We use a lower learning rate for the encoder ($10^{-6}$) to preserve its robust features while refining spatial precision. Gradient clipping (max norm 1.0) prevents training instability.
\end{itemize}

\paragraph{Training Challenges.}
Despite careful architecture design and two-stage training, the model exhibits a critical failure mode: it learns to predict the End-of-Sequence (EOS) token prematurely during training, generating incomplete FEN strings at inference. This occurs because the training objective (cross-entropy with padding) allows the model to minimize loss by predicting EOS early, then relying on padding tokens that are ignored in the loss computation. The model effectively learns to generate $\sim$20 tokens (where it predicts EOS) rather than the required 42--44 tokens for complete FEN strings.

\subsection{VLM Integration}

For both approaches, the pipeline is identical:
1.  **Input:** Chess Board Image.
2.  **Module:** The Retrieval or Generative module predicts a FEN string.
3.  **Prompting:** The VLM (GPT-4o) is prompted with both the image and the predicted FEN:
    \begin{quote}
    \textbf{Question:} [Chess Question] \\
    \textbf{FEN Context:} [Predicted FEN]
    \end{quote}

In line with explanation-augmented work such as MATE~\citep{wang2024mate}, we optionally extend the prompt with additional instructions asking the model to reason about checks, tactics, or strategy, but the core intervention is the inclusion of FEN.

\section{Experimental Setup}

\subsection{Datasets}

\textbf{Training.} For CLIP and the FEN Generator, we sample $\sim$100K chess positions from online game databases (e.g., Lichess) and puzzle sets, render them as 2D board images, and pair each with its FEN representation. This setup is analogous to the board-state corpora used in ChessBench~\citep{deepmind2024searchless} and ChessGPT~\citep{feng2023chessgpt}.

\textbf{Evaluation.} We construct a held-out test set of 12.5K positions stratified across task types (tactics, quiet positions, endgames). For some analyses we subsample task-specific sets (e.g., check detection, best-move puzzles) to mirror the evaluation structure in MATE~\citep{wang2024mate} and natural-language chess engines~\citep{jiang2023nlchess}.

\subsection{Models}

\begin{itemize}[leftmargin=*]
    \item \textbf{Retrieval Model:} Fine-tuned CLIP ViT-B/32.
    \item \textbf{Generative Model:} CLIP ViT-B/32 Encoder + Transformer Decoder, trained with the two-stage strategy.
    \item \textbf{VLM:} GPT-4o (OpenAI) used in image+text mode for question answering.
    \item \textbf{Judge:} GPT-4o-mini for automated scoring.
\end{itemize}

\subsection{Evaluation Framework}

We design 8 evaluation tasks covering different aspects of chess understanding:

\begin{table}[h]
    \centering
    \caption{Evaluation tasks for chess position understanding.}
    \begin{tabular}{lll}
        \toprule
        \textbf{Task} & \textbf{Type} & \textbf{Example} \\
        \midrule
        FEN Extraction    & Symbolic  & ``What is the FEN for this position?'' \\
        Piece Count       & Counting  & ``How many pieces does each side have?'' \\
        Check Detection   & State     & ``Is either king in check?'' \\
        Material Balance  & Evaluation& ``Who has more material?'' \\
        Best Move         & Strategic & ``What is the best move?'' \\
        Tactical Patterns & Reasoning & ``Describe any tactical patterns.'' \\
        Castling Rights   & Rules     & ``Can White castle kingside?'' \\
        Piece Localization& Spatial   & ``What piece is on e4?'' \\
        \bottomrule
    \end{tabular}
\end{table}

Scoring: We use GPT-4o-mini as an LLM judge to score VLM responses on a $0$--$1$ scale based on correctness compared to ground truth. This follows automated-judge setups used in ChessGPT~\citep{feng2023chessgpt} and other chess-LLM benchmarks.

\section{Results}

\subsection{Phase 1: Retrieval-Based Validation}

We first evaluated whether providing FEN context helps, assuming the retrieval model can find the correct (or near-correct) FEN.

\begin{table}[h]
    \centering
    \caption{Impact of Retrieved FEN on VLM Performance (Closed Set).}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Method} & \textbf{FEN Accuracy} & \textbf{VLM Reasoning Score} & \textbf{Improvement} \\
        \midrule
        Visual-Only & N/A & 0.250 & - \\
        + Retrieved FEN & 96.7\% & 0.438 & \textbf{+75.0\%} \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{Finding.} Providing the FEN significantly boosts performance, confirming our hypothesis that symbolic grounding is the key bottleneck. The largest gains appear in tasks requiring precise state understanding (check detection, FEN extraction), where a single mislocalized piece can flip correctness.

\subsection{Phase 2: Generative Performance}

We then evaluated the Generative Decoder on unseen positions where retrieval would fail. Our experiments revealed significant challenges in generating complete, accurate FEN strings.

\begin{table}[h]
    \centering
    \caption{Generative FEN Decoder Performance on Test Set (12,500 samples).}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Decoding Strategy} & \textbf{Exact Match} & \textbf{Average CER} \\
        \midrule
        Greedy Decoding (Baseline) & 0.00\% & 0.6663 \\
        Beam Search (beam=5) & 0.00\% & 0.7120 \\
        Min Length Constraint (35) & 0.00\% & 0.7204 \\
        EOS Token Masking (min=35) & 0.00\% & 0.7387 \\
        Length Penalty in Loss (v3) & 0.00\% & 1.16 \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{Training Performance.} Despite achieving low validation loss (0.0225 in Stage 1, 0.0084 in Stage 2), the model fails to generate complete FEN strings at inference time. The model consistently predicts the End-of-Sequence (EOS) token prematurely, generating only $\sim$15--20 tokens instead of the expected 42--44 tokens for complete FEN strings.

\paragraph{Key Findings.}
\begin{itemize}[leftmargin=*]
    \item \textbf{Training-Validation Gap:} Low validation loss (0.0084) does not translate to generation quality, indicating a fundamental mismatch between the training objective and the generation task.
    \item \textbf{Premature Stopping:} All decoding strategies fail to produce complete sequences, with generated FENs typically containing only 3--4 ranks instead of 8.
    \item \textbf{Inference-Time Constraints Ineffective:} Attempts to force longer generation (minimum length constraints, EOS masking) produce repetitive garbage patterns rather than valid FEN syntax.
    \item \textbf{Exposure Bias:} The model was trained with teacher forcing (always seeing ground truth), but at inference must condition on its own predictions. After $\sim$10--15 correct tokens, prediction errors compound and the model enters a degenerate generation mode.
\end{itemize}

\paragraph{Root Cause Analysis.} We identified three contributing factors:
\begin{enumerate}
    \item \textbf{Padding Strategy:} During training, sequences are padded to a fixed length (80 tokens). The model learns it can minimize loss by predicting EOS early, then relying on padding tokens (which are ignored in the loss function).
    \item \textbf{Teacher Forcing:} The model never learns to recover from its own prediction errors, as it always sees ground truth during training.
    \item \textbf{Length Mismatch:} The model is effectively trained to generate $\sim$20 tokens (where it predicts EOS), but complete FEN strings require 42--44 tokens.
\end{enumerate}

\paragraph{Example Failure Case.}
Ground truth: \texttt{r3k2r/ppb2p1p/2nqpp2/1B1p3b/Q2N4/7P/PP1N1PP1/R1B2RK1} \\
Generated (greedy): \texttt{r3k2r/1N1P02/Q02/1PK1} \\
The model correctly identifies the first rank but stops prematurely, producing an incomplete FEN that cannot be used for downstream reasoning tasks.

\subsection{Per-Task Analysis}

\begin{table}[h]
    \centering
    \caption{Per-task performance with and without FEN context.}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Task} & \textbf{Visual-Only} & \textbf{+ FEN} & $\Delta$ \\
        \midrule
        FEN Extraction   & 0.20 & 0.50 & +150\% \\
        Check Detection  & 0.00 & 1.00 & $+\infty$ \\
        Material Balance & 1.00 & 1.00 & 0\% \\
        Piece Count      & 0.00 & 0.20 & +20\% \\
        Castling Rights  & 0.80 & 0.80 & 0\% \\
        Best Move        & 0.00 & 0.00 & 0\% \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{Analysis.}
Check detection shows perfect accuracy with FEN ($1.0$ vs.\ $0.0$ without), suggesting VLMs struggle with complex piece interactions from vision alone but can exploit symbolic state, much like engine-based or explanation-augmented systems. Material balance performs well in both settings, indicating VLMs can count pieces visually. Best move remains challenging even with FEN, echoing findings from ChessLLM~\citep{zhang2025chessllm}, ChessGPT~\citep{feng2023chessgpt}, and GPT-2-based natural-language engines~\citep{jiang2023nlchess} that strong policy play requires more than accurate state estimation.

\section{Discussion}

\subsection{Why Does FEN Help?}

We hypothesize three mechanisms:

\begin{itemize}[leftmargin=*]
    \item \textbf{Disambiguation:} FEN resolves visual ambiguities (e.g., ``White pawn on e4'' vs.\ ``White bishop on e4'') and removes reliance on fine-grained pixel-level cues.
    \item \textbf{Grounding:} Textual FEN activates the VLM's chess knowledge from language pretraining and from chess-specific finetuning in related work~\citep{zhang2025chessllm,feng2023chessgpt,wang2024mate}.
    \item \textbf{Complementarity:} Vision confirms the board layout while FEN provides precise positions. In cases where CLIP mispredicts, the VLM can still cross-check against the image.
\end{itemize}

This is consistent with evidence that language explanations and structured text scaffolds improve reasoning in chess~\citep{wang2024mate} and beyond~\citep{xie2025worldmodels}.

\subsection{Lessons from Generative Model Failures}

Our extensive experimentation with generative FEN prediction revealed several important lessons for the vision-to-symbol generation community:

\paragraph{Training Metrics Can Mislead.} Our model achieved excellent validation loss (0.0084) but 0\% exact match accuracy, demonstrating that standard sequence modeling metrics (cross-entropy loss) do not necessarily correlate with generation quality for structured outputs. This echoes findings in other structured generation tasks where models optimize for local token-level accuracy rather than global sequence validity.

\paragraph{Exposure Bias is Critical.} The fundamental challenge is that teacher forcing during training creates a distribution mismatch at inference. The model never learns to recover from its own prediction errors because it always sees ground truth. After $\sim$10--15 correct tokens, small errors compound and the model enters a degenerate generation mode. This suggests that scheduled sampling or other techniques that gradually introduce model predictions during training may be necessary for successful generative models.

\paragraph{Inference-Time Constraints Are Insufficient.} We attempted multiple inference-time strategies to force longer generation (minimum length constraints, EOS token masking, beam search), but all failed because the model was never trained to generate beyond its learned distribution. Simply preventing EOS prediction does not teach the model how to continue generating valid tokens; it only produces repetitive invalid patterns. This indicates that the solution must come from training improvements, not inference-time hacks.

\paragraph{Structured Outputs Require Specialized Objectives.} The rigid syntax of FEN (exactly 8 ranks, 7 slashes, specific piece notation) suggests that sequence-level training objectives (e.g., policy gradient methods optimizing for exact match) or constrained decoding may be necessary. Standard maximum likelihood training with padding allows the model to ``cheat'' by predicting EOS early.

These findings contribute to the broader understanding of challenges in vision-to-symbol generation and suggest directions for future research in structured output generation from visual inputs.

\subsection{Relation to Chess-Specific LLMs}

Our pipeline can be viewed as a \emph{front-end} for existing chess-LLM systems:

\begin{itemize}[leftmargin=*]
    \item Text-only engines such as ChessLLM~\citep{zhang2025chessllm}, ChessGPT~\citep{feng2023chessgpt}, ALLIE~\citep{zhang2024allie}, and searchless transformers~\citep{deepmind2024searchless} could all operate on CLIP-extracted FEN instead of assuming perfect textual input.
    \item Human-move prediction models~\citep{kreiger2024predict} and Maia-style engines~\citep{maia2020} could use our visual interface to predict moves from board screenshots or video feeds.
    \item ABSA-based evaluation methods~\citep{kamlish2019sentimate,chessabsa2024} and sentiment-based engines could be applied to commentary anchored on CLIP-derived FEN in broadcast or educational settings.
\end{itemize}

In that sense, our work complements policy- and explanation-centric research by solving the ``vision bottleneck'' that prevents these methods from operating directly on images.

\subsection{Limitations and Challenges}

\paragraph{Generative Model Limitations.}
Our generative FEN decoder faces significant challenges that limit its practical deployment:
\begin{itemize}[leftmargin=*]
    \item \textbf{Zero Exact Match Accuracy:} Despite low training loss, the model achieves 0\% exact match accuracy on the test set, generating incomplete FEN strings that typically contain only 3--4 ranks instead of 8.
    \item \textbf{Exposure Bias:} The fundamental issue is that teacher forcing during training does not prepare the model for autoregressive generation at inference, where it must condition on its own (potentially erroneous) predictions.
    \item \textbf{Training Objective Mismatch:} The cross-entropy loss with padding allows the model to minimize loss by predicting EOS early, creating a disconnect between training metrics and generation quality.
    \item \textbf{Inference-Time Constraints Fail:} Attempts to force longer generation (EOS masking, minimum length constraints) produce repetitive invalid patterns rather than valid FEN syntax, indicating the model lacks the capacity to generate beyond its training distribution.
\end{itemize}

\paragraph{Partial FEN Utility.}
While complete FEN generation fails, partial FENs (even with CER $\sim$0.67) may still provide useful context for VLMs. The first 3--4 ranks often contain critical information (piece locations, king safety) that could improve VLM reasoning, though this remains an open empirical question.

\paragraph{Retrieval Model Limitations.}
The retrieval-based approach, while highly accurate (99.98\% top-1), is fundamentally limited to closed-world settings:
\begin{itemize}[leftmargin=*]
    \item \textbf{Database Dependency:} Only works for positions present in the training database, failing on novel positions.
    \item \textbf{Scalability:} Requires maintaining and searching a large FEN database, which may not be practical for all applications.
\end{itemize}

\paragraph{Task Dependency.}
Gains from FEN context are task-specific; strategic reasoning and best-move selection remain challenging even with perfect state information, consistent with prior findings~\citep{deepmind2024searchless,jiang2023nlchess}.

\paragraph{Generality.}
We evaluate on standard chess. Extensions to chess variants, 3D boards, or noisy real-world camera feeds would require domain adaptation.

\paragraph{Symbolic-Only Bias.}
By privileging FEN, we may underutilize visual cues (e.g., clock, user interface elements) that could be informative in some applications.

\subsection{Broader Impact and Connections to World Models and Games}

Our results suggest that symbolic intermediate representations can bridge the gap between vision and language in structured domains. This is aligned with broader efforts to build LLM-based world models that learn environment dynamics from logs and use them for planning and simulation~\citep{xie2025worldmodels,wang2024worldsim}. In game and narrative settings, frameworks such as PANGeA~\citep{buongiorno2024pangea} and EMO~\citep{yu2025emo} demonstrate that LLMs with memory and validation modules can maintain coherent, rule-abiding narratives in RPGs.

Potential applications include:
\begin{itemize}[leftmargin=*]
    \item \textbf{Medical imaging:} Extracting structured patient data (e.g., lesion locations) before diagnostic reasoning.
    \item \textbf{Robotics:} Converting visual scenes to symbolic state descriptions for planning.
    \item \textbf{Education and coaching:} Building explainable tutors that reason over both board images and symbolic annotations.
\end{itemize}

\section{Conclusion}

We presented a simple yet effective approach for enhancing VLM performance on chess understanding tasks by incorporating CLIP-extracted FEN representations. Our method achieves a 75\% relative improvement over vision-only baselines and plugs directly into the rapidly growing ecosystem of chess-specific LLMs and world-model approaches.

\paragraph{Key Takeaways.}
\begin{itemize}[leftmargin=*]
    \item Symbolic grounding (via FEN) significantly improves VLM spatial reasoning in structured domains like chess, as demonstrated by our retrieval-based approach achieving 75\% relative improvement.
    \item Fine-tuned CLIP models are highly effective for retrieval-based FEN matching (99.98\% top-1 accuracy), providing a practical solution for closed-world settings.
    \item Generative FEN prediction faces fundamental challenges: exposure bias and training-inference mismatch lead to premature stopping and 0\% exact match accuracy, despite low training loss.
    \item The gap between training metrics (loss) and generation quality highlights the importance of evaluating generative models on their actual output, not just training statistics.
    \item Hybrid vision--symbolic systems show promise, but current generative approaches require significant improvements before they can replace retrieval in open-world settings.
\end{itemize}

\paragraph{Future Work.}
\begin{itemize}[leftmargin=*]
    \item \textbf{Generative Model Improvements:} Address exposure bias through scheduled sampling, curriculum learning (starting with short FENs), or sequence-level training objectives. Explore alternative architectures (e.g., non-autoregressive generation) that may better handle the rigid FEN syntax.
    \item \textbf{Hybrid Approaches:} Combine retrieval (for known positions) with generative fallback (for novel positions), potentially using confidence scores to decide which method to trust.
    \item \textbf{Partial FEN Evaluation:} Systematically evaluate whether partial FENs (even with high CER) improve VLM performance, which could make the current generative model practically useful despite its limitations.
    \item \textbf{Integration:} Integrate our visual front-end with strong text-only chess engines (ChessLLM, ChessGPT, ALLIE) to evaluate full-game strength from images.
    \item \textbf{Generalization:} Extend to other structured domains (Go, molecular structures, circuit diagrams) where a concise symbolic representation exists.
    \item \textbf{Enhanced Reasoning:} Combine visual symbolic grounding with explanation-augmented reasoning~\citep{wang2024mate} and multi-agent opponent modeling~\citep{yu2025emo}.
\end{itemize}

\begin{thebibliography}{99}

\bibitem{radford2021clip}
A.~Radford et al.
\newblock Learning Transferable Visual Models From Natural Language Supervision.
\newblock In \emph{ICML}, 2021.

\bibitem{zhang2025chessllm}
Y.~Zhang et al.
\newblock Complete Chess Games Enable LLM Become A Chess Master.
\newblock \emph{arXiv:2501.17186}, 2025.

\bibitem{wang2024mate}
S.~Wang et al.
\newblock Explore the Reasoning Capability of LLMs in the Chess Testbed.
\newblock \emph{arXiv:2411.06655}, 2024.

\bibitem{kreiger2024predict}
B.~Kreiger.
\newblock Predicting Human Chess Moves with Large Language Models.
\newblock Master's Thesis, University of South Florida, 2024.

\bibitem{jiang2023nlchess}
B.~Jiang.
\newblock Building a Natural Language Chess Engine with Pretraining and Instruction Fine-Tuning.
\newblock Stanford CS224N Project Report, 2023.

\bibitem{deepmind2024searchless}
Google DeepMind.
\newblock Amortized Planning with Large-Scale Transformers (Searchless Chess).
\newblock \emph{arXiv:2402.04494}, 2024.

\bibitem{xie2025worldmodels}
K.~Xie et al.
\newblock Making Large Language Models into World Models with Precondition and Effect Knowledge.
\newblock In \emph{COLING}, 2025.

\bibitem{wang2024worldsim}
R.~Wang et al.
\newblock Can Language Models Serve as Text-Based World Simulators?
\newblock \emph{arXiv:2406.06485}, 2024.

\bibitem{zhang2024allie}
Y.~Zhang et al.
\newblock Human-aligned Chess with a Bit of Search.
\newblock \emph{arXiv:2410.03893}, 2024.

\bibitem{yu2025emo}
X.~P.~Yu, W.~Zhang, and Z.~Lu.
\newblock LLM-Based Explicit Models of Opponents for Multi-Agent Games.
\newblock In \emph{NAACL}, 2025.

\bibitem{buongiorno2024pangea}
S.~Buongiorno et al.
\newblock PANGeA: Procedural Artificial Narrative using Generative AI for Turn-Based Video Games.
\newblock \emph{arXiv:2404.19721}, 2024.

\bibitem{feng2023chessgpt}
X.~Feng et al.
\newblock ChessGPT: Bridging Policy Learning and Language Modeling.
\newblock \emph{arXiv:2306.09200}, 2023.

\bibitem{kamlish2019sentimate}
I.~Kamlish, I.~B.~Chocron, and N.~McCarthy.
\newblock SentiMATE: Learning to play Chess through Natural Language Processing.
\newblock \emph{arXiv:1907.08321}, 2019.

\bibitem{pei2025clippgs}
G.~Pei et al.
\newblock Seeing What Matters: Empowering CLIP with Patch Generation-to-Selection.
\newblock In \emph{CVPR}, 2025.

\bibitem{maia2020}
J.~McIlroy-Young et al.
\newblock Aligning Superhuman AI with Human Behavior: Chess as a Model System.
\newblock In \emph{KDD}, 2020.

\bibitem{chessabsa2024}
A.~Author et al.
\newblock Aspect-Based Sentiment Analysis for Chess Move Evaluation from Textbooks.
\newblock Technical report, 2024.

\bibitem{vicharclip2025}
V.~Padman.
\newblock Vichar-CLIP: Chess Position Identification.
\newblock GitHub Repository, \url{https://github.com/vieveks/vichar_clip_paper_cursor}, 2025.

\end{thebibliography}

\appendix

\section{Benchmark Details}
\label{app:benchmark}

\subsection*{Question Examples}

\begin{itemize}[leftmargin=*]
    \item Q1: What is the FEN (Forsyth-Edwards Notation) for this chess position?\\
          Ground Truth: \texttt{r3k2r/ppb2p1p/2nqpp2/1B1p3b/Q2N4/7P/PP1N1PP1/R1B2RK1}
    \item Q2: Is either king in check?\\
          Ground Truth: No
    \item Q3: What is the best move in this position?\\
          Ground Truth: Qh2+ (from engine analysis)
\end{itemize}

\section{Implementation}

Code and benchmarks will be made available at an anonymous repository after the review period. The current implementation is based on Vichar-CLIP for CLIP training and a simple Python harness for VLM evaluation.

\section{Qualitative Examples}

See supplementary materials for detailed VLM responses with and without FEN context, including examples of success and failure cases.

\end{document}
