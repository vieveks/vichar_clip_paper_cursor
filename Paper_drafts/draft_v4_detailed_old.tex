\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage[numbers]{natbib}
\usepackage{url}

\title{Enhancing Vision-Language Models for Chess Position Understanding via Symbolic Grounding}
\author{Vivek Padman, Prajeet Khante, Hirak Basumatory\\
\small Paper under double-blind review}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Vision-Language Models (VLMs) have shown remarkable capabilities in visual reasoning tasks, yet they struggle with precise spatial understanding required for structured domains like chess. We propose a three-phase approach to bridge this vision--text modality gap by grounding visual inputs in symbolic representations (FEN notation). First, we demonstrate via a **retrieval-based** method that providing ground-truth or retrieved FEN strings significantly enhances VLM performance, achieving 99.98\% top-1 accuracy and a 75\% relative improvement in chess understanding tasks. Second, we explore a **Generative FEN Decoder**, which reveals fundamental challenges in vision-to-symbol generation (0\% exact match due to exposure bias). Third, we introduce an **LLM-based Extraction Pipeline** that leverages state-of-the-art VLMs (GPT-4o, Claude 3.5) with consensus strategies to achieve 94\% FEN accuracy on open-world images. We demonstrate that while generative decoding remains an open research challenge, retrieval and LLM-based extraction offer complementary solutions for closed and open-world settings, respectively.
\end{abstract}

\noindent\textbf{Keywords:} Vision-Language Models, Chess AI, Symbolic Reasoning, CLIP, Multimodal Learning, Generative Decoding

\section{Introduction}

Vision-Language Models (VLMs) such as LLaVA and GPT-4V have demonstrated impressive capabilities across diverse visual reasoning tasks. However, they face challenges in domains requiring precise spatial understanding and symbolic reasoning, such as chess position analysis. While humans can easily translate a chess board image into Forsyth-Edwards Notation (FEN) --- a standard textual representation --- VLMs often struggle with this task due to the fine-grained spatial reasoning required.

Chess has rapidly become a standard testbed for probing the reasoning and planning capabilities of large language models (LLMs) and multimodal systems. Recent work shows that purely text-based LLMs trained on complete game trajectories can reach strong-amateur or even master-level strength when games are represented in FEN or UCI text formats.\footnote{E.g., ChessLLM reaches $\sim$1788 Elo against Stockfish when trained on over 20B chess tokens~\citep{zhang2025chessllm}.} Similarly, explanation-enriched datasets such as MATE demonstrate that adding strategic and tactical natural-language rationales substantially improves move-selection accuracy, surpassing strong commercial LLMs~\citep{wang2024mate}. At the same time, other work targets human move prediction~\citep{kreiger2024predict}, human-aligned play and skill calibration~\citep{zhang2024allie}, and purely searchless transformer policies trained on massive supervised datasets~\citep{deepmind2024searchless}.

Yet, despite this progress, most chess-LLM work assumes a \emph{textual} representation of the position is already available (PGN, FEN, UCI). In many realistic settings, the input is visual: a rendered board from a GUI, a video frame, or a real-world camera image. In these cases, the bottleneck is not the downstream reasoning but the \emph{vision-to-symbol} mapping that provides a clean, unambiguous board state.

Chess therefore serves as an ideal testbed for studying VLM limitations because:

Contrastive vision-language pretraining, as popularized by CLIP~\citep{radford2021clip}, learns a joint embedding space for images and text by predicting matching pairs from large-scale web data. This approach has proven remarkably effective for zero-shot classification, retrieval, and transfer across a wide range of visual domains. Subsequent work has focused on making CLIP more efficient (e.g., patch masking and patch selection strategies~\citep{pei2025clippgs}) and domain-adapting CLIP-like models to specialized settings.

Our method builds on the CLIP paradigm but extends it to **generative tasks**. While standard CLIP is excellent for retrieval, it cannot produce novel text strings for unseen visual configurations. We address this by attaching a Transformer decoder to the CLIP image encoder, allowing us to generate precise FEN strings token-by-token. This mirrors approaches in image captioning (e.g., CoCa, GIT) but specialized for the rigid syntax of chess notation.

\subsection{Chess and LLMs}

A growing body of work treats chess as a structured testbed for LLM reasoning:

\paragraph{Text-only chess LLMs.} ChessLLM~\citep{zhang2025chessllm} fine-tunes an LLM on over 20B tokens of chess games represented in FEN and move text, achieving an Elo of $\sim$1788 against Stockfish and demonstrating that high-quality, full-game supervision significantly improves strength. ChessBench and related work on amortized planning with transformers~\citep{deepmind2024searchless} show that supervised transformers trained on engine-annotated data can reach grandmaster-level performance without explicit search by predicting state- and action-values.

ChessGPT~\citep{feng2023chessgpt} bridges policy learning and language modeling through a large-scale multimodal chess dataset, introducing ChessGPT and ChessCLIP as models that jointly handle game state, policy, and language commentary. The MATE dataset~\citep{wang2024mate} further shows that enriching positions with strategy and tactic explanations yields up to $\sim$95\% move-selection accuracy and even boosts the performance of powerful black-box LLMs when explanations are provided as context.

\paragraph{Human-like and human-aligned play.} Parallel work focuses on modeling human behavior rather than optimal engine play. ALLIE~\citep{zhang2024allie} trains a multi-headed transformer to predict moves, pondering times, and resignation decisions from human game logs, achieving state-of-the-art human move prediction and near-perfect skill calibration (average gap $\sim$49 Elo). Other work explicitly targets move prediction for a single player~\citep{kreiger2024predict} or subsets of the rating spectrum~\citep{maia2020,skidanov2025behaviour}. These efforts show that LLM-like architectures can capture rich human behavior when provided with accurate textual state representations.

\paragraph{Natural-language chess engines.} Smaller models, such as the GPT-2-XL system of Jiang~\citep{jiang2023nlchess}, explore training language models to answer natural-language chess queries (e.g., legality, piece identity, evaluation) and to generate moves using instruction fine-tuning and chain-of-thought prompting. While these models exhibit promising qualitative reasoning, their move quality often lags behind specialized engines.

Our work is complementary: we assume a strong, general-purpose VLM or LLM for downstream reasoning but focus on the \emph{visual-to-symbolic} step that almost all prior work sidesteps by assuming FEN or PGN is already given.

\subsection{Chess, NLP, and Explanations}

Beyond move prediction and engine strength, several works study chess through the lens of NLP:

\begin{itemize}[leftmargin=*]
    \item \textbf{Policy + language integration:} ChessGPT~\citep{feng2023chessgpt} unifies policy learning with language modeling across game records, commentary, and instructions.
    \item \textbf{Sentiment and commentary:} SentiMATE~\citep{kamlish2019sentimate} and later ABSA-style models analyze sentiment in chess commentary to evaluate moves. Recent work extends aspect-based sentiment analysis (ABSA) to chess textbooks using player-predicate-move triples and RoBERTa-based classifiers, showing that sentiment over textual descriptions correlates meaningfully with engine evaluations.
    \item \textbf{Human move prediction with LLMs:} Kreiger~\citep{kreiger2024predict} evaluates GPT-family models and chess-specific engines on predicting human moves, finding that prompt engineering improves LLM performance but chess-specific models such as Maia still dominate.
\end{itemize}

These works support our claim that \emph{textual} representations of chess positions are powerful interfaces for LLMs. Our goal is to reliably produce such representations from images, so that text-based techniques (sentiment, explanations, policy learning) become available in purely visual settings.

\subsection{World Models, Games, and Multi-Agent Reasoning}

There is a broader trend of framing interactive environments as sequence modeling problems over text. World-model work in text-based games and synthetic environments demonstrates that transformers can approximate environment dynamics and rewards purely from logged trajectories, then serve as simulators, policies, or planners~\citep{wang2024worldsim,xie2025worldmodels}. WordPlay-style benchmarks and ByteSized corpora treat game state and actions as text, showing strong generalization and planning capabilities in interactive narratives.

In multi-agent settings, Explicit Models of Opponents (EMO)~\citep{yu2025emo} build individual LLM-based opponent models with bi-level feedback, significantly improving win rates and role inference in social deduction games. Meanwhile, narrative-focused frameworks such as PANGeA~\citep{buongiorno2024pangea} show how LLMs with memory and validation modules can maintain coherent, rule-abiding narratives in RPGs.

Chess sits at the intersection of these lines of work: it is both a complex planning problem and a social setting where human-aligned reasoning and opponent modeling matter. Our method can be viewed as providing a high-fidelity \emph{symbolic world state} (FEN) from pixels, enabling downstream world-model or multi-agent techniques to operate on structured input.

\subsection{Symbolic-Neural Hybrid Systems}

Neuro-symbolic AI aims to combine differentiable perception with discrete symbolic reasoning. Prior work in chess includes engines that incorporate sentiment or textual knowledge into evaluation functions~\citep{kamlish2019sentimate} and systems that use logic or knowledge graphs to augment neural policies.

Our approach is deliberately simple: we do not perform explicit symbolic search or theorem proving. Instead, we use symbolic representations as an \emph{input modality} to a large VLM. This follows the spirit of recent work that shows explanations and textual structure can scaffold better reasoning in LLMs~\citep{wang2024mate}, but applied to a vision-to-symbol pipeline.

\section{Method}

We propose a pipeline that augments a general-purpose VLM with a specialized "Vision-to-FEN" module. We explore two architectures for this module.

\subsection{Approach 1: Retrieval-Based FEN Matching}
\label{subsec:retrieval}

In our initial investigation, we treat FEN identification as an image--text retrieval problem.

\paragraph{Architecture.} We fine-tune a **CLIP ViT-B/32** model on pairs of (Board Image, FEN String).
\paragraph{Training.} We use the standard contrastive loss:
\begin{equation}
    \mathcal{L} = - \log \frac{\exp\left(\mathrm{sim}(I_i, T_i) / \tau\right)}{\sum_{j} \exp\left(\mathrm{sim}(I_i, T_j) / \tau\right)},
\end{equation}
where $I_i$ is the image embedding, $T_i$ is the FEN text embedding, $\mathrm{sim}(\cdot,\cdot)$ denotes cosine similarity, and $\tau$ is a temperature parameter, following CLIP~\citep{radford2021clip}.

\paragraph{Inference.} Given a query image, we compute its embedding and retrieve the nearest neighbor from a pre-computed index of FEN strings.
\paragraph{Limitation.} This method is highly accurate for positions that exist in the database (e.g., common opening lines or specific puzzles) but fails for novel positions not in the index. It serves, however, to empirically prove that *retrieved* symbolic context helps the VLM.

\subsection{Approach 2: Generative FEN Prediction}
\label{subsec:generative}

To address the closed-world limitation, we develop a generative model capable of synthesizing FEN strings for unseen positions.

\paragraph{Architecture.}
\begin{enumerate}
    \item **Encoder:** The **CLIP ViT-B/32** vision model from Approach 1, initialized with weights fine-tuned in the retrieval phase. We extract spatial patch embeddings ($7 \times 7 = 49$ tokens) rather than pooled features to preserve spatial information needed for piece localization.
    \item **Decoder:** A **Transformer Decoder** (6 layers, 8 heads, 512 dimensions) that attends to the encoder's spatial tokens and autoregressively generates the FEN string token-by-token using a character-level tokenizer.
\end{enumerate}

\paragraph{Two-Stage Training Strategy.}
\begin{itemize}
        \toprule
        \textbf{Task} & \textbf{Visual-Only} & \textbf{+ FEN} & $\Delta$ \\
        \midrule
        FEN Extraction   & 0.20 & 0.50 & +150\% \\
        Check Detection  & 0.00 & 1.00 & $+\infty$ \\
        Material Balance & 1.00 & 1.00 & 0\% \\
        Piece Count      & 0.00 & 0.20 & +20\% \\
        Castling Rights  & 0.80 & 0.80 & 0\% \\
        Best Move        & 0.00 & 0.00 & 0\% \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{Analysis.}
Check detection shows perfect accuracy with FEN ($1.0$ vs.\ $0.0$ without), suggesting VLMs struggle with complex piece interactions from vision alone but can exploit symbolic state, much like engine-based or explanation-augmented systems. Material balance performs well in both settings, indicating VLMs can count pieces visually. Best move remains challenging even with FEN, echoing findings from ChessLLM~\citep{zhang2025chessllm}, ChessGPT~\citep{feng2023chessgpt}, and GPT-2-based natural-language engines~\citep{jiang2023nlchess} that strong policy play requires more than accurate state estimation.

\section{Discussion}

\subsection{Why Does FEN Help?}

We hypothesize three mechanisms:

\begin{itemize}[leftmargin=*]
    \item \textbf{Disambiguation:} FEN resolves visual ambiguities (e.g., ``White pawn on e4'' vs.\ ``White bishop on e4'') and removes reliance on fine-grained pixel-level cues.
    \item \textbf{Grounding:} Textual FEN activates the VLM's chess knowledge from language pretraining and from chess-specific finetuning in related work~\citep{zhang2025chessllm,feng2023chessgpt,wang2024mate}.
    \item \textbf{Complementarity:} Vision confirms the board layout while FEN provides precise positions. In cases where CLIP mispredicts, the VLM can still cross-check against the image.
\end{itemize}

This is consistent with evidence that language explanations and structured text scaffolds improve reasoning in chess~\citep{wang2024mate} and beyond~\citep{xie2025worldmodels}.

\subsection{Lessons from Generative Model Failures}

Our extensive experimentation with generative FEN prediction revealed several important lessons for the vision-to-symbol generation community:

\paragraph{Training Metrics Can Mislead.} Our model achieved excellent validation loss (0.0084) but 0\% exact match accuracy, demonstrating that standard sequence modeling metrics (cross-entropy loss) do not necessarily correlate with generation quality for structured outputs. This echoes findings in other structured generation tasks where models optimize for local token-level accuracy rather than global sequence validity.

\paragraph{Exposure Bias is Critical.} The fundamental challenge is that teacher forcing during training creates a distribution mismatch at inference. The model never learns to recover from its own prediction errors because it always sees ground truth. After $\sim$10--15 correct tokens, small errors compound and the model enters a degenerate generation mode. This suggests that scheduled sampling or other techniques that gradually introduce model predictions during training may be necessary for successful generative models.

\paragraph{Inference-Time Constraints Are Insufficient.} We attempted multiple inference-time strategies to force longer generation (minimum length constraints, EOS token masking, beam search), but all failed because the model was never trained to generate beyond its learned distribution. Simply preventing EOS prediction does not teach the model how to continue generating valid tokens; it only produces repetitive invalid patterns. This indicates that the solution must come from training improvements, not inference-time hacks.

\paragraph{Structured Outputs Require Specialized Objectives.} The rigid syntax of FEN (exactly 8 ranks, 7 slashes, specific piece notation) suggests that sequence-level training objectives (e.g., policy gradient methods optimizing for exact match) or constrained decoding may be necessary. Standard maximum likelihood training with padding allows the model to ``cheat'' by predicting EOS early.

These findings contribute to the broader understanding of challenges in vision-to-symbol generation and suggest directions for future research in structured output generation from visual inputs.

\subsection{Relation to Chess-Specific LLMs}

Our pipeline can be viewed as a \emph{front-end} for existing chess-LLM systems:

\begin{itemize}[leftmargin=*]
    \item Text-only engines such as ChessLLM~\citep{zhang2025chessllm}, ChessGPT~\citep{feng2023chessgpt}, ALLIE~\citep{zhang2024allie}, and searchless transformers~\citep{deepmind2024searchless} could all operate on CLIP-extracted FEN instead of assuming perfect textual input.
    \item Human-move prediction models~\citep{kreiger2024predict} and Maia-style engines~\citep{maia2020} could use our visual interface to predict moves from board screenshots or video feeds.
    \item ABSA-based evaluation methods~\citep{kamlish2019sentimate,chessabsa2024} and sentiment-based engines could be applied to commentary anchored on CLIP-derived FEN in broadcast or educational settings.
\end{itemize}

In that sense, our work complements policy- and explanation-centric research by solving the ``vision bottleneck'' that prevents these methods from operating directly on images.

\subsection{Limitations and Challenges}

\paragraph{Generative Model Limitations.}
Our generative FEN decoder faces significant challenges that limit its practical deployment:
\begin{itemize}[leftmargin=*]
    \item \textbf{Zero Exact Match Accuracy:} Despite low training loss, the model achieves 0\% exact match accuracy on the test set, generating incomplete FEN strings that typically contain only 3--4 ranks instead of 8.
    \item \textbf{Exposure Bias:} The fundamental issue is that teacher forcing during training does not prepare the model for autoregressive generation at inference, where it must condition on its own (potentially erroneous) predictions.
    \item \textbf{Training Objective Mismatch:} The cross-entropy loss with padding allows the model to minimize loss by predicting EOS early, creating a disconnect between training metrics and generation quality.
    \item \textbf{Inference-Time Constraints Fail:} Attempts to force longer generation (EOS masking, minimum length constraints) produce repetitive invalid patterns rather than valid FEN syntax, indicating the model lacks the capacity to generate beyond its training distribution.
\end{itemize}

\paragraph{Partial FEN Utility.}
While complete FEN generation fails, partial FENs (even with CER $\sim$0.67) may still provide useful context for VLMs. The first 3--4 ranks often contain critical information (piece locations, king safety) that could improve VLM reasoning, though this remains an open empirical question.

\paragraph{Retrieval Model Limitations.}
The retrieval-based approach, while highly accurate (99.98\% top-1), is fundamentally limited to closed-world settings:
\begin{itemize}[leftmargin=*]
    \item \textbf{Database Dependency:} Only works for positions present in the training database, failing on novel positions.
    \item \textbf{Scalability:} Requires maintaining and searching a large FEN database, which may not be practical for all applications.
\end{itemize}

\paragraph{Task Dependency.}
Gains from FEN context are task-specific; strategic reasoning and best-move selection remain challenging even with perfect state information, consistent with prior findings~\citep{deepmind2024searchless,jiang2023nlchess}.

\paragraph{Generality.}
We evaluate on standard chess. Extensions to chess variants, 3D boards, or noisy real-world camera feeds would require domain adaptation.

\paragraph{Symbolic-Only Bias.}
By privileging FEN, we may underutilize visual cues (e.g., clock, user interface elements) that could be informative in some applications.

\subsection{Broader Impact and Connections to World Models and Games}

Our results suggest that symbolic intermediate representations can bridge the gap between vision and language in structured domains. This is aligned with broader efforts to build LLM-based world models that learn environment dynamics from logs and use them for planning and simulation~\citep{xie2025worldmodels,wang2024worldsim}. In game and narrative settings, frameworks such as PANGeA~\citep{buongiorno2024pangea} and EMO~\citep{yu2025emo} demonstrate that LLMs with memory and validation modules can maintain coherent, rule-abiding narratives in RPGs.

Potential applications include:
\begin{itemize}[leftmargin=*]
    \item \textbf{Medical imaging:} Extracting structured patient data (e.g., lesion locations) before diagnostic reasoning.
    \item \textbf{Robotics:} Converting visual scenes to symbolic state descriptions for planning.
    \item \textbf{Education and coaching:} Building explainable tutors that reason over both board images and symbolic annotations.
\end{itemize}

\section{Conclusion}

We presented a simple yet effective approach for enhancing VLM performance on chess understanding tasks by incorporating CLIP-extracted FEN representations. Our method achieves a 75\% relative improvement over vision-only baselines and plugs directly into the rapidly growing ecosystem of chess-specific LLMs and world-model approaches.

\paragraph{Key Takeaways.}
\begin{itemize}[leftmargin=*]
    \item Symbolic grounding (via FEN) significantly improves VLM spatial reasoning in structured domains like chess, as demonstrated by our retrieval-based approach achieving 75\% relative improvement.
    \item Fine-tuned CLIP models are highly effective for retrieval-based FEN matching (99.98\% top-1 accuracy), providing a practical solution for closed-world settings.
    \item Generative FEN prediction faces fundamental challenges: exposure bias and training-inference mismatch lead to premature stopping and 0\% exact match accuracy, despite low training loss.
    \item The gap between training metrics (loss) and generation quality highlights the importance of evaluating generative models on their actual output, not just training statistics.
    \item Hybrid vision--symbolic systems show promise, but current generative approaches require significant improvements before they can replace retrieval in open-world settings.
\end{itemize}

\paragraph{Future Work.}
\begin{itemize}[leftmargin=*]
    \item \textbf{Generative Model Improvements:} Address exposure bias through scheduled sampling, curriculum learning (starting with short FENs), or sequence-level training objectives. Explore alternative architectures (e.g., non-autoregressive generation) that may better handle the rigid FEN syntax.
    \item \textbf{Hybrid Approaches:} Combine retrieval (for known positions) with generative fallback (for novel positions), potentially using confidence scores to decide which method to trust.
    \item \textbf{Partial FEN Evaluation:} Systematically evaluate whether partial FENs (even with high CER) improve VLM performance, which could make the current generative model practically useful despite its limitations.
    \item \textbf{Integration:} Integrate our visual front-end with strong text-only chess engines (ChessLLM, ChessGPT, ALLIE) to evaluate full-game strength from images.
    \item \textbf{Generalization:} Extend to other structured domains (Go, molecular structures, circuit diagrams) where a concise symbolic representation exists.
    \item \textbf{Enhanced Reasoning:} Combine visual symbolic grounding with explanation-augmented reasoning~\citep{wang2024mate} and multi-agent opponent modeling~\citep{yu2025emo}.
\end{itemize}

\begin{thebibliography}{99}

\bibitem{radford2021clip}
A.~Radford et al.
\newblock Learning Transferable Visual Models From Natural Language Supervision.
\newblock In \emph{ICML}, 2021.

\bibitem{zhang2025chessllm}
Y.~Zhang et al.
\newblock Complete Chess Games Enable LLM Become A Chess Master.
\newblock \emph{arXiv:2501.17186}, 2025.

\bibitem{wang2024mate}
S.~Wang et al.
\newblock Explore the Reasoning Capability of LLMs in the Chess Testbed.
\newblock \emph{arXiv:2411.06655}, 2024.

\bibitem{kreiger2024predict}
B.~Kreiger.
\newblock Predicting Human Chess Moves with Large Language Models.
\newblock Master's Thesis, University of South Florida, 2024.

\bibitem{jiang2023nlchess}
B.~Jiang.
\newblock Building a Natural Language Chess Engine with Pretraining and Instruction Fine-Tuning.
\newblock Stanford CS224N Project Report, 2023.

\bibitem{deepmind2024searchless}
Google DeepMind.
\newblock Amortized Planning with Large-Scale Transformers (Searchless Chess).
\newblock \emph{arXiv:2402.04494}, 2024.

\bibitem{xie2025worldmodels}
K.~Xie et al.
\newblock Making Large Language Models into World Models with Precondition and Effect Knowledge.
\newblock In \emph{COLING}, 2025.

\bibitem{wang2024worldsim}
R.~Wang et al.
\newblock Can Language Models Serve as Text-Based World Simulators?
\newblock \emph{arXiv:2406.06485}, 2024.

\bibitem{zhang2024allie}
Y.~Zhang et al.
\newblock Human-aligned Chess with a Bit of Search.
\newblock \emph{arXiv:2410.03893}, 2024.

\bibitem{yu2025emo}
X.~P.~Yu, W.~Zhang, and Z.~Lu.
\newblock LLM-Based Explicit Models of Opponents for Multi-Agent Games.
\newblock In \emph{NAACL}, 2025.

\bibitem{buongiorno2024pangea}
S.~Buongiorno et al.
\newblock PANGeA: Procedural Artificial Narrative using Generative AI for Turn-Based Video Games.
\newblock \emph{arXiv:2404.19721}, 2024.

\bibitem{feng2023chessgpt}
X.~Feng et al.
\newblock ChessGPT: Bridging Policy Learning and Language Modeling.
\newblock \emph{arXiv:2306.09200}, 2023.

\bibitem{kamlish2019sentimate}
I.~Kamlish, I.~B.~Chocron, and N.~McCarthy.
\newblock SentiMATE: Learning to play Chess through Natural Language Processing.
\newblock \emph{arXiv:1907.08321}, 2019.

\bibitem{pei2025clippgs}
G.~Pei et al.
\newblock Seeing What Matters: Empowering CLIP with Patch Generation-to-Selection.
\newblock In \emph{CVPR}, 2025.

\bibitem{maia2020}
J.~McIlroy-Young et al.
\newblock Aligning Superhuman AI with Human Behavior: Chess as a Model System.
\newblock In \emph{KDD}, 2020.

\bibitem{chessabsa2024}
A.~Author et al.
\newblock Aspect-Based Sentiment Analysis for Chess Move Evaluation from Textbooks.
\newblock Technical report, 2024.

\bibitem{vicharclip2025}
V.~Padman.
\newblock Vichar-CLIP: Chess Position Identification.
\newblock GitHub Repository, \url{https://github.com/vieveks/vichar_clip_paper_cursor}, 2025.

\end{thebibliography}

\appendix

\section{Benchmark Details}
\label{app:benchmark}

\subsection*{Question Examples}

\begin{itemize}[leftmargin=*]
    \item Q1: What is the FEN (Forsyth-Edwards Notation) for this chess position?\\
          Ground Truth: \texttt{r3k2r/ppb2p1p/2nqpp2/1B1p3b/Q2N4/7P/PP1N1PP1/R1B2RK1}
    \item Q2: Is either king in check?\\
          Ground Truth: No
    \item Q3: What is the best move in this position?\\
          Ground Truth: Qh2+ (from engine analysis)
\end{itemize}

\section{Implementation}

Code and benchmarks will be made available at an anonymous repository after the review period. The current implementation is based on Vichar-CLIP for CLIP training and a simple Python harness for VLM evaluation.

\section{Qualitative Examples}

See supplementary materials for detailed VLM responses with and without FEN context, including examples of success and failure cases.

\end{document}
